# Nisal, Prajakta   
# 1002_174_111
# 2024_09_23
# Assignment_01_01

import numpy as np

def sigmoid(x):
    """
    Converts a continuous number to a value between range [0, 1]
    https://www.sciencedirect.com/topics/computer-science/sigmoid-function
    
    Args:
        x (float/int): input

    Returns:
        (float/int): returns value in range 0 -> 1 
    """
    
    return 1 / (1 + np.exp(-x))

def mean_square_error(Y_test, Y_predict):
    """
    Calculates Mean Square Error : small value indicates better accuracy

    Args:
        Y_test (vector - array): expected output
        Y_predict (vector - array): actual output generated by network

    Returns:
        [float/int]: returns the error (avg squared difference)
    """
    
    return np.mean((Y_test - Y_predict) ** 2)

def forward_pass(weights, layer_input, num_neurons_in_output_layer):
    """
    Performs forward propagation

    Args:
        weights (_type_): weight matrices for every layer
        layer_input (_type_): sample input 
        num_neurons_in_output_layer (_type_): final output layer neurons count

    Returns:
        network output after one forward pass
    """
    
    for weight in weights:
        
        weighted_sum = np.dot(weight, layer_input)
        activated_output = sigmoid(weighted_sum)
        
        layer_input = np.concatenate((np.array([1]), activated_output))  
        
    return layer_input  


def update_weights(weights, h, layer_input, Y_train, train_predictions, alpha, num_neurons_in_output_layer, sample_index):
    """
    updating weights using centered difference approximation to calculate partial derivatives.

    Args:
        weights (_type_): _description_
        h (_type_): _description_
        layer_input (_type_): _description_
        Y_train (_type_): _description_
        train_predictions (_type_): _description_
        alpha (_type_): _description_
        num_neurons_in_output_layer (_type_): _description_
        sample_index (_type_): _description_
    """
    
    weight_updates = [np.zeros_like(weight) for weight in weights]
    
    # layer W
    for i in range(len(weights)):
        
        # neuron Ws (no. of neurons in layer W: 0-> s)
        for j in range(weights[i].shape[0]):
            
            # weight wrt input "Wsr" (input from 0 -> r) Ch. 02
            for k in range(weights[i].shape[1]):
                
                # Use gradient descent for adjusting the weights.
                # use centered difference approximation to calculate partial derivatives.
                # (f(x + h)-f(x - h))/2*h
                original_weight = weights[i][j, k]
                
                weights[i][j, k] = original_weight + h
                forward_pass_positive_out = forward_pass(weights, layer_input, num_neurons_in_output_layer)
                positive_error = mean_square_error(Y_train[:, sample_index], forward_pass_positive_out[-num_neurons_in_output_layer:] )
                
                weights[i][j, k] = original_weight - h
                forward_pass_negative_out = forward_pass(weights, layer_input, num_neurons_in_output_layer)
                negative_error = mean_square_error(Y_train[:, sample_index], forward_pass_negative_out[-num_neurons_in_output_layer:] )
                
        
                weights[i][j, k] = original_weight
                
                gradient = (positive_error - negative_error) / (2 * h)  # h is step_size
                weight_updates[i][j, k] = (-alpha) * gradient
                
                
    for i in range(len(weights)):
        weights[i] += weight_updates[i]

def multi_layer_nn(X_train,Y_train,X_test,Y_test,layers,alpha,epochs,h=0.00001,seed=2):
    """
    Multi-layer neural network

    Args:
        X_train (_type_): [input_dimensions,nof_train_samples]
        Y_train (_type_): [input_dimensions,nof_test_samples]
        X_test (_type_): [output_dimensions,nof_train_samples]
        Y_test (_type_): [output_dimensions,nof_test_samples]
        layers (_type_): number of neurons in ith layer
        alpha (_type_): Learning rate
        epochs (_type_): epochs
        h (float, optional): step size. Defaults to 0.00001.
        seed (int, optional): random number generator for initializing weights. Defaults to 2.

    Returns:
        - Weights : [[2-d matrix]]
        - MSE : Sum((Y_Test - Y_Pred) ^ 2) / len(Y_Test)
        - Output : [dimensions, N_Y_Test]
    """

    X_train = X_train.T # [nof_train_samples, input_dimensions]
    X_test = X_test.T # [nof_test_samples, input_dimensions]
    
    # adding bias
    X_train_with_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))
    X_test_with_bias = np.hstack((np.ones((X_test.shape[0], 1)), X_test))
    
    num_neurons_in_output_layer = layers[-1]

    # Weight Initialization
    weights = initialize_weights(layers, X_train.shape[1], seed)
    
    mean_square_errors = []
    predicted_output = []
    
    # TODO: remove this code
    # check MSE before starting with training
    test_predictions = []
    for i in range(len(X_test)):
        sample_input = X_test_with_bias[i]
        test_predictions.append(forward_pass(weights, sample_input, num_neurons_in_output_layer)[-num_neurons_in_output_layer:])
        
    predicted_output = np.array(test_predictions).T    
    
    for epoch in range(epochs):
        
        # generate predictions for training data and weight adjustment
        train_predictions = []
        for i in range(len(X_train)):
            
            sample_input = X_train_with_bias[i]
            
            # update_weights(weights, h, result, Y_train, output, alpha, output_layer, i)
            update_weights(weights, h, sample_input, Y_train, train_predictions, alpha, num_neurons_in_output_layer, i)
            train_predictions.append(forward_pass(weights, sample_input, num_neurons_in_output_layer)[-num_neurons_in_output_layer:])
            
            
        # validations with test dataset (X_test, Y_test)
        test_predictions = []
        for i in range(len(X_test)):
            
            sample_input = X_test_with_bias[i]
            test_predictions.append(forward_pass(weights, sample_input, num_neurons_in_output_layer)[-num_neurons_in_output_layer:])
            
        predicted_output = np.array(test_predictions).T
        
        # calculate mean square error for every epoch
        mse = mean_square_error(Y_test, predicted_output)
        mean_square_errors.append(mse)
            
    return weights, mean_square_errors, predicted_output

def initialize_weights(layers, input_size, seed):
    """
    Initialize weights for each layer 

    Args:
        layers (_type_): number of neurons in each layer
        input_size (_type_): input dimensions
        seed (_type_): same random generator

    Returns:
        returns a weights array for all layers
    """
    
    weights = []
    
    # TODO: update name
    layers_including_input = [input_size] + layers
    
    # weight initializaion
    for i in range(1, len(layers_including_input)):
        # + 1 for bias
        np.random.seed(seed)
        layer_weights = np.random.randn(layers_including_input[i], layers_including_input[i - 1] + 1)
        weights.append(layer_weights)
        
    return weights
    

def forward_pass(weights, layer_input, num_neurons_in_output_layer):
    """
    Performs forward propagation

    Args:
        weights (_type_): weight matrices for every layer
        layer_input (_type_): sample input 
        num_neurons_in_output_layer (_type_): final output layer neurons count

    Returns:
        network output after one forward pass
    """
    
    for weight in weights:
        
        weighted_sum = np.dot(weight, layer_input)
        activated_output = sigmoid(weighted_sum)
        
        layer_input = np.concatenate((np.array([1]), activated_output))  
        
    return layer_input  


def update_weights(weights, h, layer_input, Y_train, train_predictions, alpha, num_neurons_in_output_layer, sample_index):
    """
    updating weights using centered difference approximation to calculate partial derivatives.

    Args:
        weights (_type_): _description_
        h (_type_): _description_
        layer_input (_type_): _description_
        Y_train (_type_): _description_
        train_predictions (_type_): _description_
        alpha (_type_): _description_
        num_neurons_in_output_layer (_type_): _description_
        sample_index (_type_): _description_
    """
    
    weight_updates = [np.zeros_like(weight) for weight in weights]
    
    # layer W
    for i in range(len(weights)):
        
        # neuron Ws (no. of neurons in layer W: 0-> s)
        for j in range(weights[i].shape[0]):
            
            # weight wrt input "Wsr" (input from 0 -> r) Ch. 02
            for k in range(weights[i].shape[1]):
                
                # Use gradient descent for adjusting the weights.
                # use centered difference approximation to calculate partial derivatives.
                # (f(x + h)-f(x - h))/2*h
                original_weight = weights[i][j, k]
                
                weights[i][j, k] = original_weight + h
                forward_pass_positive_out = forward_pass(weights, layer_input, num_neurons_in_output_layer)
                positive_error = mean_square_error(Y_train[:, sample_index], forward_pass_positive_out[-num_neurons_in_output_layer:] )
                
                weights[i][j, k] = original_weight - h
                forward_pass_negative_out = forward_pass(weights, layer_input, num_neurons_in_output_layer)
                negative_error = mean_square_error(Y_train[:, sample_index], forward_pass_negative_out[-num_neurons_in_output_layer:] )
                
        
                weights[i][j, k] = original_weight
                
                gradient = (positive_error - negative_error) / (2 * h)  # h is step_size
                weight_updates[i][j, k] = (-alpha) * gradient
                
                
    for i in range(len(weights)):
        weights[i] += weight_updates[i]

# def create_toy_data_nonlinear(n_samples=1000):
#     X = np.zeros((n_samples, 4))
#     X[:, 0] = np.linspace(-1, 1, n_samples)
#     X[:, 1] = np.linspace(-1, 1, n_samples)
#     X[:, 2] = np.linspace(-1, 1, n_samples)
#     X[:, 3] = np.linspace(-1, 1, n_samples)

#     y = X[:, 0]**2 + 2*X[:, 1]  - 0.5*X[:, 2] + X[:, 3]**3 + 0.3

#     # shuffle X and y
#     idx = np.arange(n_samples)
#     np.random.shuffle(idx)
#     X = X[idx]
#     y = y[idx]

#     return X.T, y[:, np.newaxis].T

# def test_check_output_shape():
#     # check if the number of nodes is being used in creating the weight matrices
#     X, y = create_toy_data_nonlinear(n_samples=110)
#     X_train = X[:, :100]
#     X_test = X[:, 100:]
#     Y_train = y[:, :100]
#     Y_test = y[:, 100:]

#     [W, err, Out] = multi_layer_nn(X_train, Y_train, X_test, Y_test, [100, 1], alpha=1e-9, epochs=0, h=1e-8, seed=2)
#     print(Out.shape == Y_test.shape)
    
# test_check_output_shape()